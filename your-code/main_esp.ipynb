{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice de contenidos\n",
    "1. Antes de empezar\n",
    "\n",
    "2. Reto 1 - Explorar el conjunto de datos\n",
    "\n",
    "    2.0.0.1 Explore los datos a vista de pájaro.\n",
    "    \n",
    "    2.0.0.2 A continuación, evalúe si las columnas de este conjunto de datos están fuertemente correlacionadas.\n",
    "\n",
    "3. Reto 2 - Eliminar la colinealidad de columnas.\n",
    "\n",
    "4. Reto 3 - Manejar los valores perdidos\n",
    "\n",
    "    4.0.0.1 En las celdas siguientes, trate los valores que faltan en el conjunto de datos. Recuerde comentar los fundamentos de sus decisiones.\n",
    "    \n",
    "    4.0.0.2 De nuevo, examine el número de valores que faltan en cada columna.\n",
    "\n",
    "5. Reto 4 - Manejo de datos categóricos WHOIS_*\n",
    "    \n",
    "    5.0.0.1 En las celdas siguientes, fije los valores de los países como se ha indicado anteriormente.\n",
    "    \n",
    "    5.0.0.2 Si un número limitado de valores representa la mayoría de los datos, podemos conservar estos valores principales y volver a etiquetar todos los demás valores poco frecuentes.\n",
    "    \n",
    "    5.0.0.3 Después de comprobarlo, mantengamos los 10 valores principales de la columna y reetiquetemos las demás columnas con OTROS.\n",
    "    \n",
    "    5.0.0.4 En la siguiente celda, elimine ['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'].\n",
    "\n",
    "6. Reto 5 - Manejar los datos categóricos restantes y convertirlos en ordinales\n",
    "    \n",
    "    6.0.0.1 URL es fácil. Simplemente la eliminaremos porque tiene demasiados valores únicos que no hay forma de consolidar.\n",
    "    \n",
    "    6.0.0.2 Imprima el conteo de valores únicos de CHARSET. Puede ver que sólo hay unos pocos valores únicos. Así que podemos dejarlo como está.\n",
    "    \n",
    "    6.0.0.3 Antes de pensar en su propia solución, no lea las instrucciones que vienen a continuación.\n",
    "\n",
    "7. Desafío 6 - Modelado, predicción y evaluación\n",
    "    \n",
    "    7.0.0.1 En este laboratorio probaremos dos modelos diferentes y compararemos nuestros resultados.\n",
    "    \n",
    "    7.0.0.2 Nuestro segundo algoritmo es DecisionTreeClassifier.\n",
    "    \n",
    "    7.0.0.3 Crearemos otro modelo DecisionTreeClassifier con max_depth=5.\n",
    "\n",
    "8. Bonus Challenge - Escalado de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antes de empezar:\n",
    "- Lee el archivo README.md\n",
    "- Comenta todo lo que puedas y utiliza los recursos del archivo README.md\n",
    "- ¡Feliz aprendizaje!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este laboratorio, exploraremos un conjunto de datos que describe sitios web con diferentes características y los etiqueta como benignos o maliciosos. Utilizaremos algoritmos de aprendizaje supervisado para averiguar qué patrones de características es probable que tengan los sitios web maliciosos y utilizaremos nuestro modelo para predecir sitios web maliciosos.\n",
    "\n",
    "Sus características serán:\n",
    "\n",
    "+ URL: es la identificación anónima de la URL analizada en el estudio\n",
    "+ URL_LENGTH: es el número de caracteres de la URL\n",
    "+ NUMBER_SPECIAL_CHARACTERS: es el número de caracteres especiales identificados en la URL, como, «/», «%», «#», «&», «. “, ”=»\n",
    "+ CHARSET: es un valor categórico y su significado es el estándar de codificación de caracteres (también llamado juego de caracteres).\n",
    "+ SERVER: es un valor categórico y su significado es el sistema operativo del servidor obtenido de la respuesta del paquete.\n",
    "+ CONTENT_LENGTH: representa el tamaño del contenido de la cabecera HTTP.\n",
    "+ WHOIS_COUNTRY: es una variable categórica, sus valores son los países que obtuvimos de la respuesta del servidor (en concreto, nuestro script utilizó la API de Whois).\n",
    "+ WHOIS_STATEPRO: es una variable categórica, sus valores son los estados que obtuvimos de la respuesta del servidor (en concreto, nuestro script utilizó la API de Whois).\n",
    "+ WHOIS_REGDATE: Whois proporciona la fecha de registro del servidor, por tanto, esta variable tiene valores de fecha con formato DD/MM/AAAA HH:MM\n",
    "+ WHOIS_UPDATED_DATE: A través del Whois obtenemos la última fecha de actualización del servidor analizado\n",
    "+ TCP_CONVERSATION_EXCHANGE: Esta variable es el número de paquetes TCP intercambiados entre el servidor y nuestro cliente honeypot\n",
    "+ DIST_REMOTE_TCP_PORT: es el número de puertos detectados y diferentes a TCP\n",
    "+ REMOTE_IPS: esta variable tiene el número total de IPs conectadas al honeypot\n",
    "+ APP_BYTES: es el número de bytes transferidos\n",
    "+ SOURCE_APP_PACKETS: paquetes enviados desde el honeypot al servidor\n",
    "+ REMOTE_APP_PACKETS: paquetes recibidos del servidor\n",
    "+ APP_PACKETS: número total de paquetes IP generados durante la comunicación entre el honeypot y el servidor.\n",
    "+ DNS_QUERY_TIMES: número de paquetes DNS generados durante la comunicación entre el honeypot y el servidor.\n",
    "+ TYPE: es una variable categórica, sus valores representan el tipo de página web analizada, en concreto, 1 es para sitios web maliciosos y 0 para sitios web benignos\n",
    "\n",
    "# Desafío 1 - Explorar el conjunto de datos\n",
    "\n",
    "Empecemos explorando el conjunto de datos. Primero carga el archivo de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv('../website.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore los datos a vista de pájaro.\n",
    "\n",
    "Ahora ya deberías estar muy familiarizado con los procedimientos, así que no te daremos las instrucciones paso a paso. Reflexiona sobre lo que hiciste en los laboratorios anteriores y explora el conjunto de datos.\n",
    "\n",
    "Cosas que buscarás:\n",
    "\n",
    "* ¿Qué aspecto tiene el conjunto de datos?\n",
    "* ¿Cuáles son los tipos de datos?\n",
    "* ¿Qué columnas contienen las características de los sitios web?\n",
    "* ¿Qué columna contiene la característica que vamos a predecir? ¿Cuál es el código de los sitios web benignos frente a los maliciosos?\n",
    "* ¿Necesitamos transformar alguna de las columnas de categórica a ordinal? En caso afirmativo, ¿cuáles son esas columnas?\n",
    "\n",
    "Siéntete libre de añadir celdas adicionales para tus exploraciones. Asegúrate de comentar lo que descubras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the dataset looks like?\n",
    "websites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data types?\n",
    "websites.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns contain the features of the websites?\n",
    "X = websites.drop(columns=['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns # variables predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "y = websites['Type'] # target\n",
    "y.value_counts() # codificación binaria. Benign -> 0, Malicious -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for bening 1 for maliciuos websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para hacernos una idea de los datos que hay, de los valores nulos, y de los valores que toma cada columna, hacemos value_counts y unique, así como len(unique()) para todos los casos\n",
    "for col in X.columns:\n",
    "    print(f\"{col}\\n\")\n",
    "    print(f\"'{col}' value_counts: {X[col].value_counts()}\")\n",
    "    print(f\"'{col}' unique: {X[col].unique()}\")\n",
    "    print(f\"'{col}' len(unique): {len(X[col].unique())}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites['CHARSET'] = websites['CHARSET'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHARSET\n",
    "X['CHARSET_'] = X['CHARSET'].str.lower().str.strip()\n",
    "X['CHARSET_'].unique()\n",
    "X.drop(columns=['CHARSET'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "charset_onehot = pd.get_dummies(X['CHARSET_'], drop_first=False).astype(int)\n",
    "X_encoded = pd.concat([X.drop(columns='CHARSET_'), charset_onehot], axis=1)\n",
    "X_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A continuación, evalúe si las columnas de este conjunto de datos están fuertemente correlacionadas.\n",
    "\n",
    "En el laboratorio de aprendizaje supervisado Mushroom que hicimos recientemente, mencionamos que nos preocupa si nuestro conjunto de datos tiene columnas fuertemente correlacionadas porque si es el caso tenemos que elegir ciertos algoritmos de ML en lugar de otros. Ahora tenemos que evaluar esto para nuestro conjunto de datos.\n",
    "\n",
    "Por suerte, la mayoría de las columnas de este conjunto de datos son ordinales, lo que nos facilita mucho las cosas. En las siguientes celdas, evalúe el nivel de colinealidad de los datos.\n",
    "\n",
    "Aquí tienes algunas indicaciones generales que puede consultar para completar este paso:\n",
    "\n",
    "1. Crea una matriz de correlaciones utilizando las columnas numéricas del conjunto de datos.\n",
    "\n",
    "2. Crea un mapa de calor utilizando `seaborn` para visualizar qué columnas tienen una alta colinealidad.\n",
    "\n",
    "3. Comenta qué columnas podría necesitar eliminar debido a la alta colinealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = websites.select_dtypes(include='number').columns.tolist()\n",
    "cols_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_num = websites[cols_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "correlation_matrix = websites_num.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', square=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este es un ejemplo para conocer la importancia de las características usando un modelo ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = websites.select_dtypes(include='number').drop('Type', axis=1)\n",
    "y = websites.Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb = xgb.XGBClassifier(random_state=random_state)\n",
    "xgb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = xgb.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.barh(X.columns[sort_idx],xgb.feature_importances_[sort_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    En el gráfico anterior podemos ver las características con menor peso en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío 2 - Eliminar la colinealidad de columnas.\n",
    "\n",
    "En el mapa de calor que ha creado, deberías haber visto al menos 3 columnas que pueden eliminarse debido a la alta colinealidad. Elimina estas columnas del conjunto de datos.\n",
    "\n",
    "Ten en cuenta que debes eliminar el menor número posible de columnas. No tienes que eliminar todas las columnas a la vez. En su lugar, intenta eliminar una columna y, a continuación, vuelve a elaborar el mapa térmico para determinar si deben eliminarse columnas adicionales. Cuando el conjunto de datos ya no contenga columnas correlacionadas en más de un 90%, puedes parar. Además, ten en cuenta que cuando dos columnas tienen una alta colinealidad, sólo necesitas eliminar una de ellas, pero no ambas.\n",
    "\n",
    "En las celdas de abajo, elimina tantas columnas como puedas para eliminar la alta colinealidad en el conjunto de datos. Asegúrate de comentar tu camino para que se pueda conocer tu razonamiento, lo que permitirá dar feedback. Al final, vuelve a imprimir el mapa de calor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_num[(websites_num['SOURCE_APP_PACKETS'] - websites_num['APP_PACKETS']) == 0] # Es la misma info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las correlaciones altas de 0.99 o 1 de la matriz de correlación\n",
    "websites_num.drop(columns=[\"URL_LENGTH\", \"SOURCE_APP_PACKETS\", \"REMOTE_APP_BYTES\", \"APP_PACKETS\", \"REMOTE_APP_PACKETS\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "correlation_matrix = websites_num.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', square=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "X = websites_num.drop('Type', axis=1)\n",
    "y = websites_num.Type\n",
    "\n",
    "xgb = xgb.XGBClassifier(random_state=random_state)\n",
    "xgb.fit(X,y)\n",
    "\n",
    "sort_idx = xgb.feature_importances_.argsort()\n",
    "\n",
    "plt.barh(X.columns[sort_idx],xgb.feature_importances_[sort_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 3 - Manejar los valores que faltan\n",
    "\n",
    "El siguiente paso sería manejar los valores faltantes. **Comenzamos examinando el número de valores que faltan en cada columna.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop the rows with missing data\n",
    "drop_cols = [\"URL_LENGTH\", \"SOURCE_APP_PACKETS\", \"REMOTE_APP_BYTES\", \"APP_PACKETS\", \"REMOTE_APP_PACKETS\"]\n",
    "websites.drop(columns=drop_cols, inplace=True)\n",
    "100*(websites.isnull().sum() / len(websites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firts we will drop the columns with more than 50% of missing data\n",
    "# Drop 'CONTENT_LENGTH' \n",
    "websites_num.drop(columns=['CONTENT_LENGTH'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"SOURCE_APP_PACKETS\", \"REMOTE_APP_BYTES\", \"APP_PACKETS\", \"REMOTE_APP_PACKETS\", \"CONTENT_LENGTH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop the rows with missing data\n",
    "websites.drop(columns=\"CONTENT_LENGTH\", inplace=True)\n",
    "websites.isnull().sum() / len(websites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De nuevo, examina el número de valores que faltan en cada columna. \n",
    "\n",
    "    Si todos están limpios, procede. Si no, vuelve atrás y haz más limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 4 - Manejar datos categóricos `WHOIS_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varias columnas categóricas que necesitamos manejar. Estas columnas son:\n",
    "\n",
    "* URL\n",
    "* CHARSET\n",
    "* SERVIDOR\n",
    "* PAÍS\n",
    "* «WHOIS_STATEPRO\n",
    "* WHOIS_REGDATE\n",
    "* WHOIS_UPDATED_DATE\n",
    "\n",
    "La forma de tratar las columnas de cadena es siempre caso por caso. Empecemos trabajando con `WHOIS_COUNTRY`. Tus pasos son:\n",
    "\n",
    "1. Enumera los valores únicos de `WHOIS_COUNTRY`.\n",
    "1. Consolide los valores de país con códigos de país coherentes. Por ejemplo, los siguientes valores se refieren al mismo país y deben utilizar un código de país coherente:\n",
    "    * `CY` y `Cyprus`.\n",
    "    * US y US\n",
    "    * SE y SE\n",
    "    * GB, Reino Unido y GB, Reino Unido.\n",
    "\n",
    "#### En las celdas de abajo, fija los valores de los países como se indica arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "good_country = {'None':'None', \n",
    "                'US':'US', \n",
    "                'SC':'SC', \n",
    "                'GB':'UK', \n",
    "                'UK':'UK', \n",
    "                'RU':'RU', \n",
    "                'AU':'AU', \n",
    "                'CA':'CA',\n",
    "                'PA':'PA',\n",
    "                'se':'SE', \n",
    "                'IN':'IN',\n",
    "                'LU':'LU', \n",
    "                'TH':'TH', \n",
    "                \"[u'GB'; u'UK']\":'UK', \n",
    "                'FR':'FR',\n",
    "                'NL':'NL',\n",
    "                'UG':'UG', \n",
    "                'JP':'JP', \n",
    "                'CN':'CN', \n",
    "                'SE':'SE',\n",
    "                'SI':'SI', \n",
    "                'IL':'IL', \n",
    "                'ru':'RU', \n",
    "                'KY':'KY', \n",
    "                'AT':'AT', \n",
    "                'CZ':'CZ', \n",
    "                'PH':'PH', \n",
    "                'BE':'BE', \n",
    "                'NO':'NO', \n",
    "                'TR':'TR', \n",
    "                'LV':'LV',\n",
    "                'DE':'DE', \n",
    "                'ES':'ES', \n",
    "                'BR':'BR', \n",
    "                'us':'US', \n",
    "                'KR':'KR', \n",
    "                'HK':'HK', \n",
    "                'UA':'UA', \n",
    "                'CH':'CH', \n",
    "                'United Kingdom':'UK',\n",
    "                'BS':'BS', \n",
    "                'PK':'PK', \n",
    "                'IT':'IT', \n",
    "                'Cyprus':'CY', \n",
    "                'BY':'BY', \n",
    "                'AE':'AE', \n",
    "                'IE':'IE', \n",
    "                'UY':'UY', \n",
    "                'KG':'KG'}\n",
    "\n",
    "websites.WHOIS_COUNTRY = websites.WHOIS_COUNTRY.apply(lambda x : good_country[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que hemos fijado los valores de los países, ¿podemos convertir ahora esta columna en ordinal?\n",
    "\n",
    "Todavía no. Si reflexionas, en los laboratorios anteriores sobre cómo manejamos las columnas categóricas, probablemente recuerdes que acabamos eliminando muchas de esas columnas porque hay demasiados valores únicos. Demasiados valores únicos en una columna no es deseable en el aprendizaje automático porque hace que la predicción sea inexacta. Pero hay soluciones bajo ciertas condiciones. Una de las condiciones solucionables es:\n",
    "\n",
    "#### Si un número limitado de valores representa la mayoría de los datos, podemos conservar estos valores principales y volver a etiquetar todos los demás valores poco frecuentes.\n",
    "\n",
    "La columna `WHOIS_COUNTRY` resulta ser este caso. Puedes comprobarlo imprimiendo un gráfico de barras de los `value_counts` en la siguiente celda para verificarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def print_bar_plot(x,y):\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.bar(x, y)\n",
    "    return plt.show()\n",
    "\n",
    "def print_barh_plot(x,y):\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.barh(x, y)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bar_plot(websites.WHOIS_COUNTRY.value_counts().index,websites.WHOIS_COUNTRY.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Después de verificar, ahora vamos a mantener los 10 primeros valores de la columna y volver a etiquetar otras columnas con `OTHER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_countries10 = websites['WHOIS_COUNTRY'].value_counts().head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites['WHOIS_COUNTRY'] = websites['WHOIS_COUNTRY'].apply(lambda x: x if x in top_countries10 else 'OTHER')\n",
    "websites['WHOIS_COUNTRY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_barh_plot(websites.WHOIS_COUNTRY.value_counts(ascending=True).index,websites.WHOIS_COUNTRY.value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que se ha cambiado la etiqueta `WHOIS_COUNTRY`, ya no necesitamos `WHOIS_STATEPRO` porque los valores de los estados o provincias pueden dejar de ser relevantes. Eliminaremos esta columna.\n",
    "\n",
    "Además, también eliminaremos `WHOIS_REGDATE` y `WHOIS_UPDATED_DATE`. Se trata de las fechas de registro y actualización de los dominios del sitio web. No son de nuestra incumbencia.\n",
    "\n",
    "#### En la siguiente celda, elimina `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.drop(columns=['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 5 - Manejar los datos categóricos restantes y convertirlos en ordinales\n",
    "\n",
    "Ahora vuelve a imprimir los `dtypes` de los datos. Además de `WHOIS_COUNTRY` que ya hemos arreglado, deberían quedar 3 columnas categóricas: `URL`, `CHARSET`, y `SERVER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `URL` es fácil. Simplemente lo eliminaremos porque tiene demasiados valores únicos que no hay forma de consolidar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.drop(columns='URL', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imprime el recuento de valores únicos de `CHARSET`. Usted ve que hay sólo unos pocos valores únicos. Así que podemos dejarlo como está."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites['CHARSET'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER` es un poco más complicado. Imprime sus valores únicos y piensa cómo puedes consolidar esos valores.\n",
    "\n",
    "#### Antes de pensar en tu propia solución, no leas las instrucciones que vienen a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites['SERVER'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque hay tantos valores únicos en la columna `SERVER`, en realidad sólo hay 3 tipos principales de servidores: Microsoft, Apache y Nginx. Simplemente comprueba si cada valor de `SERVER` contiene alguno de esos tipos de servidor y vuelve a etiquetarlos. Para los valores `SERVER` que no contengan ninguna de esas subcadenas, etiquétalos con `Other`.\n",
    "\n",
    "Al final, la columna «SERVIDOR» sólo debe contener 4 valores únicos: `Microsoft`, `Apache`, `nginx`, y `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "types = ['Microsoft', 'Apache', 'nginx']\n",
    "websites['SERVER'] = websites['SERVER'].apply(lambda x: next((t for t in types if t in x), 'Other'))\n",
    "websites['SERVER'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, todos nuestros datos categóricos están fijados ahora. **Vamos a convertirlos en datos ordinales usando la función `get_dummies` de Pandas ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)). Asegúrate de eliminar las columnas categóricas pasando `drop_first=True` a `get_dummies` ya que no las necesitamos. **Además, asigna los datos con valores ficticios a una nueva variable `website_dummy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_dummy = pd.get_dummies(websites, drop_first=True).astype(int)\n",
    "website_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "correlation_matrix = website_dummy.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', square=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix Dummies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, inspeccione `website_dummy` para asegurarse de que los datos y tipos son los previstos - no debería haber ninguna columna categórica en este punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_dummy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío 6 - Modelado, predicción y evaluación\n",
    "\n",
    "Comenzaremos esta sección dividiendo los datos en train y test. **Nombra tus 4 variables `X_entrenamiento`, `X_prueba`, `y_entrenamiento` y `y_prueba`. Selecciona el 80% de los datos para entrenar y el 20% para probar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = website_dummy.drop('Type', axis=1)\n",
    "y = website_dummy['Type']\n",
    "# Split in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "# Check train and test sizes\n",
    "print(X.shape, X_train.shape, X_test.shape)\n",
    "print(y.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En este laboratorio, probaremos dos modelos diferentes y compararemos nuestros resultados.\n",
    "\n",
    "El primer modelo que utilizaremos en este laboratorio es la regresión logística. Ya hemos aprendido sobre la regresión logística como algoritmo de clasificación. En la celda de abajo, cargue `LogisticRegression` de scikit-learn e inicialice el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fitting a multiple linear model\n",
    "lr = LogisticRegression() # Create the LogisticRegression estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, ajustamos el modelo a nuestros datos de entrenamiento. Ya hemos separado nuestros datos en 4 partes. Utilízalos en tu modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the fitting\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Print coeficient and intercept of the regression\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, importamos `confusion_matrix` y `accuracy_score` de `sklearn.metrics` y ajustamos nuestros datos de prueba. Asigna los datos ajustados a `y_pred` e imprime la matriz de confusión y la puntuación de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Make prediction\n",
    "# y_test_pred = lr.predict(X_test)\n",
    "y_test_prob = lr.predict_proba(X_test)[:, 1]  # Probabilidad para la clase positiva\n",
    "y_test_pred = (y_test_prob >= 0.5).astype(int)  # Predicción binaria\n",
    "\n",
    "# y_train_pred = lr.predict(X_train)\n",
    "y_train_prob = lr.predict_proba(X_train)[:, 1]  # Probabilidad para la clase positiva\n",
    "y_train_pred = (y_train_prob >= 0.5).astype(int)  # Predicción binaria\n",
    "\n",
    "# Calcular matriz de confusión y precisión\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Matriz de confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precisión:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_dummy['Type'].value_counts(normalize=True) # Está muy desbalanceado, por eso predice todo 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(y, y_pred):\n",
    "    # Generate the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Adding tick marks and labels for clarity\n",
    "    classes = np.unique(y)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Adding text annotations to each cell in the confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Assume y and y_pred are defined elsewhere in your code\n",
    "print(\"Classification accuracy:\", metrics.accuracy_score(y_test, y_test_pred))\n",
    "plot_confusion_matrix(y_test, y_test_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué opinas del rendimiento del modelo? Escribe tus conclusiones a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuestro segundo algoritmo es DecisionTreeClassifier\n",
    "\n",
    "Aunque no es necesario, vamos a ajustar un modelo utilizando los datos de entrenamiento y luego probar el rendimiento del modelo utilizando los datos de prueba. Empezaremos cargando `DecisionTreeClassifier` de scikit-learn y luego inicializando y ajustando el modelo. Empezaremos con un modelo donde max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree  # For decision tree models\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(random_state=42,max_depth=3)\n",
    "dt.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir utilizando el modelo entrenado\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calcular la matriz de confusión y la precisión\n",
    "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Matriz de confusión del Árbol de Decisión:\")\n",
    "print(conf_matrix_dt)\n",
    "print(\"Precisión del Árbol de Decisión:\", accuracy_dt)\n",
    "\n",
    "# print(\"Classification accuracy:\", metrics.accuracy_score(y_test, y_pred_dt))\n",
    "plot_confusion_matrix(y_test, y_pred_dt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a crear otro modelo DecisionTreeClassifier con max_depth=5. \n",
    "Inicia y ajusta el modelo de abajo e imprime la matriz de confusión y la puntuación de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree  # For decision tree models\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(random_state=42,max_depth=5)\n",
    "dt.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir utilizando el modelo entrenado\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calcular la matriz de confusión y la precisión\n",
    "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Matriz de confusión del Árbol de Decisión:\")\n",
    "print(conf_matrix_dt)\n",
    "print(\"Precisión del Árbol de Decisión:\", accuracy_dt)\n",
    "\n",
    "# print(\"Classification accuracy:\", metrics.accuracy_score(y_test, y_pred_dt))\n",
    "plot_confusion_matrix(y_test, y_pred_dt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Has observado una mejora en la matriz de confusión al aumentar max_depth a 5? ¿Has observado una mejora en la puntuación de precisión? Escribe tus conclusiones a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión: No, no hay mejora al usar max_depth 5 pero sí que hay mejora respecto al LogisticRegression que predecía todo Type \"0\". Como ejercicio extra pruebo RandomForestClassifier y vemos que funciona mucho mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = metrics.accuracy_score(y_pred, y_test)\n",
    "acc\n",
    "\n",
    "# Calcular la matriz de confusión y la precisión\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Matriz de confusión del Random Forest:\")\n",
    "print(conf_matrix_dt)\n",
    "print(\"Precisión del Random Forest:\", accuracy_rf)\n",
    "\n",
    "# print(\"Classification accuracy:\", metrics.accuracy_score(y_test, y_pred_dt))\n",
    "plot_confusion_matrix(y_test, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Add your conclusion here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Escalado de características\n",
    "\n",
    "La resolución de problemas en el aprendizaje automático es iterativa. Puede mejorar la predicción de su modelo con diversas técnicas (aunque hay un punto óptimo para el tiempo que invierte y la mejora que obtiene). Ahora sólo has completado una iteración del análisis ML. Hay más iteraciones que puedes realizar para introducir mejoras. Para poder hacerlo, necesitarás conocimientos más profundos en estadística y dominar más técnicas de análisis de datos. En este bootcamp, no tenemos tiempo para alcanzar ese objetivo avanzado. Pero harás esfuerzos constantes después del bootcamp para conseguirlo finalmente.\n",
    "\n",
    "Sin embargo, ahora sí queremos que aprendas una de las técnicas avanzadas que se llama *feature scaling*. La idea del escalado de características es estandarizar/normalizar el rango de variables independientes o características de los datos. Esto puede hacer que los valores atípicos sean más evidentes para que pueda eliminarlos. Este paso debe realizarse durante el Desafío 6 después de dividir los datos de entrenamiento y de prueba, ya que no desea dividir los datos de nuevo, lo que hace imposible comparar los resultados con y sin el escalado de características. Para conceptos generales sobre el escalado de características, haga clic [aquí](https://en.wikipedia.org/wiki/Feature_scaling). Para profundizar, haga clic [aquí](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "En la siguiente celda, intente mejorar la precisión de predicción de su modelo mediante el escalado de características. Una librería que puedes utilizar es `sklearn.preprocessing.RobustScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)). Utilizarás `RobustScaler` para ajustar y transformar tu `X_train`, y luego transformar `X_test`. Utilizarás la regresión logística para ajustar y predecir tus datos transformados y obtener la puntuación de precisión de la misma manera. Compare la puntuación de precisión con sus datos normalizados con los datos de precisión anteriores. ¿Se ha producido alguna mejora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your comments here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-mida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
